{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mload_ext\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mautoreload\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mautoreload\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from utils import get_model_values_df, DATASET_INFO, sample_data\n",
    "from baselines import labeled_data_alone\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# --- Configuration ---\n",
    "seed = 42\n",
    "nl = 20\n",
    "nu = 1000\n",
    "sim = True\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset = 'CivilComments'\n",
    "subgroups = None          \n",
    "n_classes = DATASET_INFO[dataset]['n_classes']\n",
    "model_names = DATASET_INFO[dataset]['model_names'] \n",
    "subgroup_list = subgroups.split(',') if subgroups is not None else None\n",
    "\n",
    "# --- Load DataFrame with model predictions ---\n",
    "dataset_df = get_model_values_df(dataset, model_names)\n",
    "train_dataset_df = dataset_df.sample(frac=0.5, random_state=seed)\n",
    "test_dataset_df = dataset_df[~dataset_df.index.isin(train_dataset_df.index)]\n",
    "\n",
    "# --- Sample labeled and unlabeled data from train set ---\n",
    "sampled_data, sampled_labels, sampled_true_labels, sampled_data_df = sample_data(\n",
    "    train_dataset_df, nl, nu, model_names, seed, n_classes\n",
    ")\n",
    "\n",
    "labeled_idxs = np.where(sampled_labels != -1)[0]\n",
    "unlabeled_idxs = np.where(sampled_labels == -1)[0]\n",
    "\n",
    "# Assign group memberships\n",
    "sampled_groups = [np.array(['global'] * (nl + nu))]\n",
    "test_groups = [np.array(['global'] * len(test_dataset_df))]\n",
    "\n",
    "if subgroup_list:\n",
    "    for subgroup in subgroup_list:\n",
    "        sampled_groups.append(np.array(sampled_data[subgroup].values))\n",
    "        test_groups.append(np.array(test_dataset_df[subgroup].values))\n",
    "\n",
    "# Sanity checks\n",
    "assert len(labeled_idxs) == nl, \"Number of labeled examples does not match N_LABELED.\"\n",
    "assert len(unlabeled_idxs) == nu, \"Number of unlabeled examples does not match N_UNLABELED.\"\n",
    "\n",
    "print(f\"Loaded {len(train_dataset_df)} train samples and {len(test_dataset_df)} test samples\")\n",
    "print(f\"Sampled {nl} labeled and {nu} unlabeled examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_labeled_data = (\n",
    "    sampled_data[labeled_idxs],\n",
    "    [d[labeled_idxs] for d in sampled_groups],\n",
    "    sampled_labels[labeled_idxs]\n",
    ")\n",
    "\n",
    "estimation_unlabeled_data = (\n",
    "    sampled_data[unlabeled_idxs],\n",
    "    [d[unlabeled_idxs] for d in sampled_groups],\n",
    "    sampled_labels[unlabeled_idxs]\n",
    ")\n",
    "\n",
    "\n",
    "test_data = (\n",
    "    test_dataset_df[model_names].values,\n",
    "    test_groups,\n",
    "    test_dataset_df['label'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled data alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics using labeled baseline\n",
    "method_config = {'dataset': dataset, 'subgroups': subgroups}\n",
    "metrics_df = labeled_data_alone(estimation_labeled_data, method_config)\n",
    "\n",
    "# Add model names to the results\n",
    "metrics_df['model'] = metrics_df['model_idx'].apply(lambda x: model_names[x])\n",
    "\n",
    "# Display key metrics\n",
    "print(\"\\n=== Estimated Performance Metrics ===\")\n",
    "print(metrics_df[['model', 'acc', 'ece', 'auc', 'auprc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SSME_KDE \n",
    "\n",
    "# Compute metrics using labeled baseline\n",
    "method_config = {'dataset': dataset, 'subgroups': subgroups}\n",
    "ssme_metrics_df = SSME_KDE(estimation_labeled_data, estimation_unlabeled_data, method_config)\n",
    "\n",
    "# Add model names to the results\n",
    "ssme_metrics_df['model'] = ssme_metrics_df['model_idx'].apply(lambda x: model_names[x])\n",
    "\n",
    "# Display key metrics\n",
    "print(\"\\n=== Estimated Performance Metrics ===\")\n",
    "print(ssme_metrics_df[['model', 'ece', 'auc', 'auprc', 'acc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_metrics_df = labeled_data_alone(test_data, method_config)\n",
    "gt_metrics_df['model'] = gt_metrics_df['model_idx'].apply(lambda x: model_names[x])\n",
    "\n",
    "print(\"\\n=== Ground Truth Performance Metrics ===\")\n",
    "print(gt_metrics_df[['model', 'ece', 'auc', 'auprc', 'acc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSME_fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
