{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loaded 66891 train samples and 66891 test samples\n",
      "Sampled 20 labeled and 1000 unlabeled examples\n"
     ]
    }
   ],
   "source": [
    "# Auto-reload extension for development (optional)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from utils import get_model_values_df, DATASET_INFO, sample_data\n",
    "from baselines import labeled_data_alone\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# ---------------- Configuration ----------------\n",
    "seed = 42\n",
    "n_labeled = 20\n",
    "n_unlabeled = 1000\n",
    "dataset = 'CivilComments'\n",
    "subgroups = None  # Optionally specify comma-separated subgroup names as a string\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "n_classes = DATASET_INFO[dataset]['n_classes']\n",
    "model_names = DATASET_INFO[dataset]['model_names']\n",
    "subgroup_list = subgroups.split(',') if subgroups is not None else None\n",
    "\n",
    "# ---------------- Data Loading ----------------\n",
    "# Load DataFrame with model predictions\n",
    "dataset_df = get_model_values_df(dataset, model_names)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_dataset_df = dataset_df.sample(frac=0.5, random_state=seed)\n",
    "test_dataset_df = dataset_df.drop(train_dataset_df.index)\n",
    "\n",
    "# ---------------- Data Sampling ----------------\n",
    "# Sample labeled and unlabeled data from train set\n",
    "sampled_data, sampled_labels, sampled_true_labels, sampled_data_df = sample_data(\n",
    "    train_dataset_df, n_labeled, n_unlabeled, model_names, seed, n_classes\n",
    ")\n",
    "\n",
    "labeled_idxs = np.where(sampled_labels != -1)[0]\n",
    "unlabeled_idxs = np.where(sampled_labels == -1)[0]\n",
    "\n",
    "# ---------------- Group Metadata ----------------\n",
    "# Start with 'global' group for all examples\n",
    "sampled_groups = [np.full(n_labeled + n_unlabeled, 'global', dtype=object)]\n",
    "test_groups = [np.full(len(test_dataset_df), 'global', dtype=object)]\n",
    "\n",
    "if subgroup_list:\n",
    "    for subgroup in subgroup_list:\n",
    "        sampled_groups.append(sampled_data[subgroup].values.astype(str))\n",
    "        test_groups.append(test_dataset_df[subgroup].values.astype(str))\n",
    "\n",
    "assert len(labeled_idxs) == n_labeled, \"Mismatch in number of labeled examples.\"\n",
    "assert len(unlabeled_idxs) == n_unlabeled, \"Mismatch in number of unlabeled examples.\"\n",
    "\n",
    "print(f\"Loaded {len(train_dataset_df)} train samples and {len(test_dataset_df)} test samples\")\n",
    "print(f\"Sampled {n_labeled} labeled and {n_unlabeled} unlabeled examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_labeled_data = (\n",
    "    sampled_data[labeled_idxs],\n",
    "    [d[labeled_idxs] for d in sampled_groups],\n",
    "    sampled_labels[labeled_idxs]\n",
    ")\n",
    "\n",
    "estimation_unlabeled_data = (\n",
    "    sampled_data[unlabeled_idxs],\n",
    "    [d[unlabeled_idxs] for d in sampled_groups],\n",
    "    sampled_labels[unlabeled_idxs]\n",
    ")\n",
    "\n",
    "test_data = (\n",
    "    test_dataset_df[model_names].values,\n",
    "    test_groups,\n",
    "    test_dataset_df['label'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled data alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Estimated Performance Metrics ===\n",
      "        model  acc      ece      auc    auprc\n",
      "    alg_CORAL 0.75 0.148426 0.781250 0.429861\n",
      "      alg_ERM 0.90 0.102048 1.000000 1.000000\n",
      "      alg_IRM 0.80 0.211246 0.937500 0.679167\n",
      "alg_ERM_seed1 0.90 0.102431 0.968750 0.916667\n",
      "alg_ERM_seed2 0.90 0.106434 1.000000 1.000000\n",
      "alg_IRM_seed1 0.85 0.158549 0.921875 0.645833\n",
      "alg_IRM_seed2 0.80 0.219561 0.921875 0.645833\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics using labeled baseline\n",
    "method_config = {'dataset': dataset, 'subgroups': subgroups}\n",
    "metrics_df = labeled_data_alone(estimation_labeled_data, method_config)\n",
    "\n",
    "# Add model names to the results\n",
    "metrics_df['model'] = metrics_df['model_idx'].apply(lambda x: model_names[x])\n",
    "\n",
    "# Display key metrics\n",
    "print(\"\\n=== Estimated Performance Metrics ===\")\n",
    "print(metrics_df[['model', 'acc', 'ece', 'auc', 'auprc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:02<00:09,  1.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated priors:  [np.float64(0.857), np.float64(0.143)]\n",
      "\n",
      "=== Estimated Performance Metrics ===\n",
      "        model      ece      auc    auprc      acc\n",
      "    alg_CORAL 0.081280 0.930762 0.640757 0.862745\n",
      "      alg_ERM 0.043703 0.962893 0.802357 0.934314\n",
      "      alg_IRM 0.063187 0.962031 0.764166 0.923529\n",
      "alg_ERM_seed1 0.050335 0.959139 0.772711 0.929412\n",
      "alg_ERM_seed2 0.032324 0.968018 0.808547 0.940196\n",
      "alg_IRM_seed1 0.060823 0.943967 0.784146 0.926471\n",
      "alg_IRM_seed2 0.071643 0.962270 0.745536 0.917647\n"
     ]
    }
   ],
   "source": [
    "from model import SSME_KDE \n",
    "\n",
    "# Compute metrics using labeled baseline\n",
    "method_config = {'dataset': dataset, 'subgroups': subgroups}\n",
    "ssme_metrics_df = SSME_KDE(estimation_labeled_data, estimation_unlabeled_data, method_config)\n",
    "\n",
    "# Add model names to the results\n",
    "ssme_metrics_df['model'] = ssme_metrics_df['model_idx'].apply(lambda x: model_names[x])\n",
    "\n",
    "# Display key metrics\n",
    "print(\"\\n=== Estimated Performance Metrics ===\")\n",
    "print(ssme_metrics_df[['model', 'ece', 'auc', 'auprc', 'acc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ground Truth Performance Metrics ===\n",
      "        model      ece      auc    auprc      acc\n",
      "    alg_CORAL 0.059626 0.863927 0.399186 0.882914\n",
      "      alg_ERM 0.060149 0.939604 0.725134 0.923263\n",
      "      alg_IRM 0.105903 0.916018 0.660259 0.881957\n",
      "alg_ERM_seed1 0.061202 0.938858 0.726129 0.923039\n",
      "alg_ERM_seed2 0.049073 0.942639 0.737038 0.922516\n",
      "alg_IRM_seed1 0.096068 0.911152 0.667215 0.892258\n",
      "alg_IRM_seed2 0.101714 0.920637 0.656295 0.887399\n"
     ]
    }
   ],
   "source": [
    "gt_metrics_df = labeled_data_alone(test_data, method_config)\n",
    "gt_metrics_df['model'] = gt_metrics_df['model_idx'].apply(lambda x: model_names[x])\n",
    "\n",
    "print(\"\\n=== Ground Truth Performance Metrics ===\")\n",
    "print(gt_metrics_df[['model', 'ece', 'auc', 'auprc', 'acc']].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSME_fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
