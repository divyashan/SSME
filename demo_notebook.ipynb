{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loaded 66891 train samples and 66891 test samples\n",
      "Sampled 20 labeled and 1000 unlabeled examples\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from utils import get_model_values_df, DATASET_INFO, sample_data\n",
    "from baselines import labeled_data_alone\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# --- Configuration ---\n",
    "seed = 42\n",
    "nl = 20\n",
    "nu = 1000\n",
    "sim = True\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset = 'CivilComments'\n",
    "subgroups = None          \n",
    "n_classes = DATASET_INFO[dataset]['n_classes']\n",
    "model_names = DATASET_INFO[dataset]['model_names'] \n",
    "subgroup_list = subgroups.split(',') if subgroups is not None else None\n",
    "\n",
    "# --- Load DataFrame with model predictions ---\n",
    "dataset_df = get_model_values_df(dataset, model_names)\n",
    "train_dataset_df = dataset_df.sample(frac=0.5, random_state=seed)\n",
    "test_dataset_df = dataset_df[~dataset_df.index.isin(train_dataset_df.index)]\n",
    "\n",
    "# --- Sample labeled and unlabeled data from train set ---\n",
    "sampled_data, sampled_labels, sampled_true_labels, sampled_data_df = sample_data(\n",
    "    train_dataset_df, nl, nu, model_names, seed, n_classes\n",
    ")\n",
    "\n",
    "labeled_idxs = np.where(sampled_labels != -1)[0]\n",
    "unlabeled_idxs = np.where(sampled_labels == -1)[0]\n",
    "\n",
    "# Assign group memberships\n",
    "sampled_groups = [np.array(['global'] * (nl + nu))]\n",
    "test_groups = [np.array(['global'] * len(test_dataset_df))]\n",
    "\n",
    "if subgroup_list:\n",
    "    for subgroup in subgroup_list:\n",
    "        sampled_groups.append(np.array(sampled_data[subgroup].values))\n",
    "        test_groups.append(np.array(test_dataset_df[subgroup].values))\n",
    "\n",
    "# Sanity checks\n",
    "assert len(labeled_idxs) == nl, \"Number of labeled examples does not match N_LABELED.\"\n",
    "assert len(unlabeled_idxs) == nu, \"Number of unlabeled examples does not match N_UNLABELED.\"\n",
    "\n",
    "print(f\"Loaded {len(train_dataset_df)} train samples and {len(test_dataset_df)} test samples\")\n",
    "print(f\"Sampled {nl} labeled and {nu} unlabeled examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_labeled_data = (\n",
    "    sampled_data[labeled_idxs],\n",
    "    [d[labeled_idxs] for d in sampled_groups],\n",
    "    sampled_labels[labeled_idxs]\n",
    ")\n",
    "\n",
    "estimation_unlabeled_data = (\n",
    "    sampled_data[unlabeled_idxs],\n",
    "    [d[unlabeled_idxs] for d in sampled_groups],\n",
    "    sampled_labels[unlabeled_idxs]\n",
    ")\n",
    "\n",
    "\n",
    "test_data = (\n",
    "    test_dataset_df[model_names].values,\n",
    "    test_groups,\n",
    "    test_dataset_df['label'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled data alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Estimated Performance Metrics ===\n",
      "        model  acc      ece      auc    auprc\n",
      "    alg_CORAL 0.75 0.148426 0.781250 0.429861\n",
      "      alg_ERM 0.90 0.102048 1.000000 1.000000\n",
      "      alg_IRM 0.80 0.211246 0.937500 0.679167\n",
      "alg_ERM_seed1 0.90 0.102431 0.968750 0.916667\n",
      "alg_ERM_seed2 0.90 0.106434 1.000000 1.000000\n",
      "alg_IRM_seed1 0.85 0.158549 0.921875 0.645833\n",
      "alg_IRM_seed2 0.80 0.219561 0.921875 0.645833\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics using labeled baseline\n",
    "method_config = {'dataset': dataset, 'subgroups': subgroups}\n",
    "metrics_df = labeled_data_alone(estimation_labeled_data, method_config)\n",
    "\n",
    "# Add model names to the results\n",
    "metrics_df['model'] = metrics_df['model_idx'].apply(lambda x: model_names[x])\n",
    "\n",
    "# Display key metrics\n",
    "print(\"\\n=== Estimated Performance Metrics ===\")\n",
    "print(metrics_df[['model', 'acc', 'ece', 'auc', 'auprc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_preds shape:  (1020, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:09<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated priors:  [0.866, 0.134]\n",
      "\n",
      "=== Estimated Performance Metrics ===\n",
      "        model      ece      auc    auprc      acc\n",
      "    alg_CORAL 0.075502 0.929264 0.587280 0.863725\n",
      "      alg_ERM 0.040767 0.969579 0.794922 0.941176\n",
      "      alg_IRM 0.069773 0.960916 0.745116 0.916667\n",
      "alg_ERM_seed1 0.047401 0.970522 0.783655 0.940196\n",
      "alg_ERM_seed2 0.024967 0.973233 0.817928 0.945098\n",
      "alg_IRM_seed1 0.073047 0.942631 0.723688 0.915686\n",
      "alg_IRM_seed2 0.080486 0.956676 0.728252 0.908824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from model import SSME_KDE \n",
    "\n",
    "# Compute metrics using labeled baseline\n",
    "method_config = {'dataset': dataset, 'subgroups': subgroups}\n",
    "ssme_metrics_df = SSME_KDE(estimation_labeled_data, estimation_unlabeled_data, method_config)\n",
    "\n",
    "# Add model names to the results\n",
    "ssme_metrics_df['model'] = ssme_metrics_df['model_idx'].apply(lambda x: model_names[x])\n",
    "\n",
    "# Display key metrics\n",
    "print(\"\\n=== Estimated Performance Metrics ===\")\n",
    "print(ssme_metrics_df[['model', 'ece', 'auc', 'auprc', 'acc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ground Truth Performance Metrics ===\n",
      "        model      ece      auc    auprc      acc\n",
      "    alg_CORAL 0.059625 0.863927 0.399186 0.882914\n",
      "      alg_ERM 0.060140 0.939604 0.725134 0.923263\n",
      "      alg_IRM 0.105904 0.916018 0.660259 0.881957\n",
      "alg_ERM_seed1 0.061192 0.938858 0.726129 0.923039\n",
      "alg_ERM_seed2 0.049065 0.942639 0.737038 0.922516\n",
      "alg_IRM_seed1 0.096066 0.911152 0.667215 0.892258\n",
      "alg_IRM_seed2 0.101711 0.920637 0.656295 0.887399\n"
     ]
    }
   ],
   "source": [
    "gt_metrics_df = labeled_data_alone(test_data, method_config)\n",
    "gt_metrics_df['model'] = gt_metrics_df['model_idx'].apply(lambda x: model_names[x])\n",
    "\n",
    "print(\"\\n=== Ground Truth Performance Metrics ===\")\n",
    "print(gt_metrics_df[['model', 'ece', 'auc', 'auprc', 'acc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
